# Preliminary Methods and Approach

There were many approaches that we considered in attempting to make the most accurate predictions.

The first step of the project was creating a function that could accept 
a specified method, 
and return the results of training on data windos of size up to a specified value of $n$. This
function was successful and laid the foundation for comparing different
variable selection and modeling processes. The function works by
iterating over every combination of round, year, and $n$ selected for the fitting procedure, and then trains an XGBoosting model over the $n$ previous races to predict podium finish outcomes on the next race in sequence. After predictions are made for each race, the function summarizes performance metrics for each value of $n$. 
The predictions are classified by assigning the drivers with the 3 highest probabilities
derived from the model to the podium class and the rest to the non-podium class. 
The metric that we thought was most indicative of performance was the average F1 score.
F1 score is calculated by $\frac{2*P*R}{P+R}$ where P is the precision (the accuracy of
podium predictions) and R is recall (the true positive rate). An F1 score greater than
0.5 indicates that the model is better than random guessing. F1 score was better than
using accuracy since accuracy would naturally be pretty high for every model with the worse case
of getting 14/17 non-podium predictions correct. 

Once we had the ability to see how F1 score varied for each value of n we immediately saw a trend
starting to form. As n increased, F1 score would generally increase up to a certain point. After
reaching that point, larger values of $n$ typically yielded the same or lower F1 scores. These were encouraging
results as it appeared there was a "sweet spot" value of $n$.

The next step was determining what type of model and feature selection was the best.

The first type of model that we fit was a logistic regression model with manually selected features.
This approach did not yield high F1 scores. One issue that we ran into was encoding interaction terms.

We also experimented with decision trees. The decision trees showed some promising results and could
generate high F1 scores for individual races. Unfortunately, the decision trees were prone to overfitting which limited their effectiveness.

At this point we wanted to try out automatic feature selection. This was done using XGBoost. The thought here
was also that the models could benefit from interaction features implemented by XGBoost. This approach however also
did not perform very well. After fitting XGBoost over a series of 
data window sizes, we found that a model which included all of the 
features typically performed worse relative a model with far 
fewer features. This indicated that features were not adequately being
selected, since their effect could have been ignored in a full model
if it were. 

Additionally, we observed higher F1 scores after implementing synthetic minority over-sampling or SMOTE.
Up to this point, our classification problem had been imbalanced. Only 3/20 or 15% of results were positive (podium) classifications.
This means that our models were biased toward negative (non-podium) classifications. By using SMOTE, the minority (positive)
classifications are randomly resampled from the minority distribution until the number of positive and negative classifications
are more balanced. This balanced classification technique improved performance by 15-20% in F1 score. 

# Selected Approach and Methodology

This process ultimately led us to our final approach which was using an xg boosting regressor for predicting 
the classification value (0 or 1 - the result was the mean value of 0 and 1 for each region
selected by the regressor - which amounts to the probability of a positive classification). 
In hindsight, we could have also used an XGB classifier with predict probabilities to 
rank the top 3 most likely podium finishers and classify podium finishes in that way. 
Then, we manually selected features by running models for each feature one at a time, checking if they improved the average F1 score of predictions over the 2024 race result data in addition to
features which had performed best previously. 
The features we ultimately settled on are listed in the results section of this report. 

Additionally, we utilized a variation of SMOTE (SMOTE-NC) to improve the balance of the 
training data distribution. This helped to improve the performance of our methods as seen by the resulting 20% improvement in F1 scores compared to not using this technique.

Our experimental evaluations led us to determine that 6 to 8 races was the 
optimal number of races to train on for predicting the results of the next race in 
sequence. Additional information about these methods can be seen in the results section. 

<!--
Logistic regression
Decision trees - overfitting

SMOTE - imbalance classification, only 15% of results were actual positive classification, biased toward negative classifications, improved performance by 15-20% in F1 score
xg_boost - automatically implements interaction features

automatic feature selection did not work well (from xg_boost)

manual forward selection - picking a few features, adding random features until we could no longer improve performance

Final:
manual selection
xg boosting regressor selecting top 3 probabilities 

-->