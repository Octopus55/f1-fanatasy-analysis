{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Logistic Regression Modeling for Race and Qualifying Outcomes (2010-present)\"\n",
        "subtitle: \"STAT 3255 Final Project\"\n",
        "author: Joshua Lee\n",
        "format: \n",
        "  revealjs:\n",
        "    theme: moon\n",
        "    transition: slide\n",
        "    embed-resources: true\n",
        "    slide-number: true\n",
        "    preview-links: auto\n",
        "  execute:\n",
        "    warning: false\n",
        "    error: false\n",
        "  output:\n",
        "    quarto::quarto_presentation:\n",
        "      scrollable: true\n",
        "---"
      ],
      "id": "1183021b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Author: Joshua Lee\n",
        "* Submission Date: 2024-04-15 / 11:59pm\n",
        "\n",
        "# Preliminary Analysis\n",
        "\n",
        "Load the data into scope"
      ],
      "id": "6059ce9c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = pd.read_feather(\"../data/clean_model_data.feather\")\n",
        "data2 = data.loc[data['year'] >= 2022]\n",
        "data2 = data2.reset_index()\n",
        "\n",
        "# eliminate new variables from the dataset\n",
        "base_data = data.drop(['ref_year','ref_name','strt_len_mean','strt_len_q1',\n",
        "                       'strt_len_q1','strt_len_median','strt_len_q3',\n",
        "                       'strt_len_max','strt_len_min','str_len_std',\n",
        "                       'avg_track_spd','max_track_spd','min_track_spd',\n",
        "                       'std_track_spd','corner_spd_mean','corner_spd_q1',\n",
        "                       'corner_spd_median','corner_spd_q3','corner_spd_max',\n",
        "                       'corner_spd_min','num_slow_corners','num_fast_corners',\n",
        "                       'num_corners','circuit_len','regulation_id','engine_reg',\n",
        "                       'tire_reg','aero_reg','chastech_reg','sporting_reg',\n",
        "                       'pitstop_reg','years_since_major_cycle','is_major_reg',\n",
        "                       'cycle'], axis=1)"
      ],
      "id": "85f6877e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Look at a correlation matrix to identify the most obvious patterns"
      ],
      "id": "44916c69"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import seaborn as sns\n",
        "\n",
        "numeric = data.select_dtypes(include=['int64', 'float64'])\n",
        "corr = numeric.corr()\n",
        "sns.heatmap(corr)"
      ],
      "id": "8b400b78",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, the variables which demonstrate the highest correlation to\n",
        "finishing position (positionOrder) are the following\n",
        "\n",
        "+ `prev_driver_position`\n",
        "+ `prev_constructor_postion`\n",
        "+ `quali_position`\n",
        "\n",
        "Additionally, `prev_driver_position` and `prev_constructor_position` are strongly\n",
        "correlated to the qualifying position. These will definitely be variables we want\n",
        "to include in the final model. \n",
        "\n",
        "This is to be expected given the results obtained by prior works. The \n",
        "track descriptive data is unlikely to be useful unless we include interactions\n",
        "across drivers, constructors, the year, and potentially the regulation cycle. \n",
        "(this is especially true when dealing with linear models)\n",
        "\n",
        "The first thing we want to do however is develop a preprocessing scheme for \n",
        "converting all of our extremely important categorical variables into meaningful \n",
        "numeric data. I use a standard one-hot encoding technique to accomplish this \n",
        "task.\n",
        "\n",
        "> define a helper function for studying categorical variables"
      ],
      "id": "9d9d06d2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "def get_features(data:pd.DataFrame, features=[], select=False):\n",
        "    if len(features) == 0: features = ['driverId', 'constructorId']\n",
        "    encoder = OneHotEncoder()\n",
        "    one_hot = encoder.fit_transform(data[features])\n",
        "    feature_names = encoder.get_feature_names_out()\n",
        "    df_features = pd.DataFrame(one_hot.toarray(), columns=feature_names)\n",
        "    \n",
        "    if select==True:\n",
        "        df_study = pd.concat([data[['positionOrder','quali_position']], df_features], axis=1)\n",
        "    else:\n",
        "        df_study = pd.concat([data, df_features], axis=1)\n",
        "\n",
        "    return df_study"
      ],
      "id": "97d5f7d9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_encoded_data(dat:pd.DataFrame):\n",
        "    # we need lists of these variables so we can create interactions between the encoded\n",
        "    # categoricals and the other variables of interest\n",
        "    driver_vars = ['driverId_{}'.format(id) for id in dat['driverId'].unique()] \n",
        "    construct_vars = ['constructorId_{}'.format(id) for id in dat['constructorId'].unique()]\n",
        "    cycle_vars = ['cycle_{}'.format(id) for id in dat['cycle'].unique()]\n",
        "\n",
        "    # test track characteristics - corner_spd_mean, num_fast_corners, num_slow_corners, strt_len_max\n",
        "    # test all constructors\n",
        "    # test regulation types - engine_reg, aero_reg, years_since_major_cycle\n",
        "    #       engine_reg * years_since_major_cycle + engine_reg\n",
        "    #       aero_reg * years_since_major_cycle + aero_reg\n",
        "    encoded_dat = get_features(dat, ['constructorId','driverId', 'cycle'], select=False)\n",
        "    return driver_vars, construct_vars, cycle_vars, encoded_dat\n",
        "\n",
        "driver_vars, construct_vars, cycle_vars, encoded_dat = get_encoded_data(data2)"
      ],
      "id": "685fed24",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the function on drivers and cosntructors"
      ],
      "id": "55cbbc12"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "driver_construct_dat = get_features(data, ['driverId', 'constructorId', 'cycle'], select=True)\n",
        "corr = driver_construct_dat.corr()\n",
        "sns.heatmap(corr)"
      ],
      "id": "37d1d0e7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because there are so many drivers in the data, it is hard to visualize who are\n",
        "which drivers are the most predictively significant (for finishing position). \n",
        "Instead, we select the most important drivers and constructors - specifically\n",
        "we identify drivers with a negative or positive correlation to their finishing\n",
        "position which is greater than 0.1, or less than -0.1"
      ],
      "id": "d0725034"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x = corr.loc['positionOrder'].T\n",
        "x = x[(x > 0.2) | (x < -0.2)]\n",
        "x"
      ],
      "id": "8ba0ada5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From this output, we identify driver Id's 1, 3, 10, 20, 39, 822, and 830. Let's \n",
        "first examine these individuals:"
      ],
      "id": "4e3b9a6b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# read in drivers data for reference\n",
        "drivers = pd.read_csv(\"../data/drivers.csv\")\n",
        "\n",
        "values = {1,6,9,20,39,822,830}\n",
        "subset = drivers.loc[drivers['driverId'].isin(values), 'driverRef']\n",
        "subset"
      ],
      "id": "141448f2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The names are useful, but we might also want to examine some relevant racing finishing\n",
        "position statistics:"
      ],
      "id": "b13ab765"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "recent_drivers = data[['driverId', 'constructorId', 'positionOrder', 'quali_position']]\n",
        "\n",
        "for id in recent_drivers['driverId'].unique():\n",
        "    recent_drivers.loc[recent_drivers['driverId'] == id,'avg_fp'] = recent_drivers.loc[recent_drivers['driverId'] == id,'positionOrder'].mean()\n",
        "    recent_drivers.loc[recent_drivers['driverId'] == id,'std_fp'] = recent_drivers.loc[recent_drivers['driverId'] == id,'positionOrder'].std()\n",
        "    recent_drivers.loc[recent_drivers['driverId'] == id,'avg_quali'] = recent_drivers.loc[recent_drivers['driverId'] == id,'quali_position'].mean()\n",
        "    recent_drivers.loc[recent_drivers['driverId'] == id,'std_quali'] = recent_drivers.loc[recent_drivers['driverId'] == id,'quali_position'].std()\n",
        "\n",
        "recent_drivers = recent_drivers[['driverId', \n",
        "                                 'avg_fp',\n",
        "                                 'std_fp',\n",
        "                                 'avg_quali',\n",
        "                                 'std_quali']].drop_duplicates()\n",
        "\n",
        "driver_stats = pd.merge(drivers, recent_drivers, on='driverId', how='inner')"
      ],
      "id": "7f54b601",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "driver_stats.loc[driver_stats['driverId'].isin(values),\n",
        "                 ['driverId', 'driverRef', 'avg_fp', 'std_fp','avg_quali', 'std_quali']]"
      ],
      "id": "6c61d6ac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you compare this output with the table output with the correlation table output, \n",
        "you will notice that the observed correlations are reasonable. Namely, Hamilton, who has \n",
        "an extremely high average finishing position also demonstrates a strong negative correlation\n",
        "with finishing position. The same pattern holds true of Verstappen, Rosberg, Vettel, and \n",
        "Valteri Bottas. On the other hand, drivers with high average finishing positions were \n",
        "positively correlated to the finishing position. This also makes sense\n",
        "\n",
        "Notes on Interaction Effect Studies\n",
        "\n",
        "+ it might be interesting to test multiple versions of the year. One\n",
        "  version of the year variable can be scaled and used as a measure of time. \n",
        "  For example, how do drivers develop over time, or year in the sport. In \n",
        "  this sense it might be useful to treat the year as a numeric value. \n",
        "  However, regulation cycles and the development of cars tend to be more \n",
        "  categorical in nature. This might be challenging to make use of when \n",
        "  we are dealing with new years though. For exampl, in 2024, we don't have \n",
        "  too much data to build upon. Namely, we only have 24 races throughout the year, \n",
        "  and the time when predictions will be most useful occur well before we get to \n",
        "  the point when all of the data is available. \n",
        "+ years since last major regulation change - this will be useful as a \n",
        "  numeric variable, but must be interacted with the teams specifically to \n",
        "  be of any significance to the model. Leaving the variable alone won't be\n",
        "  particularly useful to us. However, we may be able to use the regulation \n",
        "  standards and their nature as a proxy for this. These categorical \n",
        "  descriptors are not dependent upon a fixed categorical variable. Instead, \n",
        "  they can be used to describe any given year of F1 competition\n",
        "  + we also want to interact `is_major_reg` or `engine_reg`, `tire_reg`, and \n",
        "    `areo_reg` with the teams because different teams may perform \n",
        "    differently given different type of regulation sets. This is especially\n",
        "    the case when comparing the performance of teams. For example, Mercedes\n",
        "    performed extremely well at the beginning of the 2014 regulation cycle. \n",
        "    This year marked a strong departure from the previous season's results. \n",
        "    However, when the new aerodynamic regulations began in 2022, the \n",
        "    competitive order of the field changed dramatically, with Red Bull becoming\n",
        "    the dominant force in F1, alongside Ferrari. \n",
        "+ constructor, round, years_since_major_cycle, is_major_reg, regulation type variables\n",
        "  + may be interesting to investigate the second order interactions\n",
        "  + third order interaction between team, round, and years since major cycle \n",
        "    may also be quite useful in this case\n",
        "  + it may be unecessary to investigate `is_major_reg` directly since the \n",
        "    `years_since_major_cycle=0` if this variable is set as `1`\n",
        "+ weather (wind, precipitation, etc.), constructor\n",
        "  + It may be the case that different cosntructors can develop cars which are\n",
        "    better or worse suited to performance under different conditions. \n",
        "    For example, teams like Williams have struggled with changable wind conditions.\n",
        "    If the wind is strong, it is possible that their performances may be weaker than\n",
        "    otherwise would be the case. \n",
        "+ weather (precipitation, wind), driver\n",
        "  + The driver may also have an effect on performances in wet conditions. Namely, \n",
        "    drivers such as Lando Norris, Lewis Hamilton, Max Verstappen, and others have\n",
        "    been known to perform better in the wet relative to their rivals, even \n",
        "    when considering existing performance differentials.\n",
        "+ driver crossed with previous points may also be a strong predictor. This could\n",
        "  also be the case for the constructor points. \n",
        "\n",
        "We need to be selective about which variables we choose to incorporate \n",
        "in the final model however, since there is a limited amount of data \n",
        "with which to train an overall model. A deep learning model may be able \n",
        "to extract the important effects given a greater amount of data. \n",
        "\n",
        "The strategy we use here is as follows:\n",
        "\n",
        "+ define all of the interaction variables that we are interested in \n",
        "  studying\n",
        "+ run select K best algorithm to identify the most significant\n",
        "  features to retain for our model.\n",
        "+ train and validate (n-folds) for each of value of $k$ used. Namely, \n",
        "  if we generate 5 different values of $k$, $\\{1,2,4,8,10,20,30\\}$,\n",
        "  we want to train and evaluate each of the models constructed and\n",
        "  compare their performance.\n",
        "\n",
        "The original goal of our study was to examine the influence of the\n",
        "constructor and regulation cycle. As such, these are the primary \n",
        "factors I will include as my focus here. Primarily, I want to \n",
        "two terms:\n",
        "\n",
        "$$\n",
        "t_{4} = \\text{constructor} \\times \\text{track characteristic} \\times \\text{round} \\times \\text{regulation type} \\\\\n",
        "t_{3} = \\text{constructor} \\times \\text{track characteristic} \\times \\text{weather condition}\n",
        "$$\n",
        "\n",
        "Each of the track characteristic and regulation type variables have a subset of other variables\n",
        "for which we need to test different combinations. Let's start by building up the second order\n",
        "interactions though and seeing how those correlate to race outcomes:"
      ],
      "id": "7d80cf98"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# we need lists of these variables so we can create interactions between the encoded\n",
        "# categoricals and the other variables of interest\n",
        "driver_vars = ['driverId_{}'.format(id) for id in data['driverId'].unique()] \n",
        "construct_vars = ['constructorId_{}'.format(id) for id in data['constructorId'].unique()]\n",
        "cycle_vars = ['cycle_{}'.format(id) for id in data['cycle'].unique()]\n",
        "\n",
        "# test track characteristics - corner_spd_mean, num_fast_corners, num_slow_corners, strt_len_max\n",
        "# test all constructors\n",
        "# test regulation types - engine_reg, aero_reg, years_since_major_cycle\n",
        "#       engine_reg * years_since_major_cycle + engine_reg\n",
        "#       aero_reg * years_since_major_cycle + aero_reg\n",
        "encoded_dat = get_features(data, ['constructorId','driverId', 'cycle'], select=False)"
      ],
      "id": "8cf4ae82",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we create the new variables"
      ],
      "id": "b08c567a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def add_interaction(data, vars=[], drivers=[], constructors=[]):\n",
        "    '''\n",
        "    Args:\n",
        "    - vars --------- list of the additional variables to include in \n",
        "                     each interaction term. For example, if vars held\n",
        "                     track_min_speed and engine_reg, then we would \n",
        "                     generate interaction terms\n",
        "                     driver * constructor * engine_reg * track_min_speed\n",
        "                     This is always assumed to have at least one \n",
        "                     value held in it\n",
        "    - drivers ------ list of drivers to create an interaction term for\n",
        "    - constructors - list of constructors to create an interaction term for\n",
        "\n",
        "    We use copies of the numpy arrays every time because not doing so \n",
        "    will overwrite the original data which would mess everything up\n",
        "    '''\n",
        "    data2 = data.copy()\n",
        "\n",
        "    if len(drivers) == 0: drivers.append(\"any_driver\")\n",
        "    if len(constructors) == 0: constructors.append(\"any_constructor\")\n",
        "    for i in range(len(drivers)):\n",
        "        for j in range(i,len(constructors)):\n",
        "            # set the initial value for the array\n",
        "            interact = data[vars[0]].copy()\n",
        "\n",
        "            v_string = \"\"\n",
        "            # handle using driver as an interaction\n",
        "            if drivers[i] != \"any_driver\":\n",
        "                interact *= data[drivers[i]].copy()\n",
        "                drive_val = drivers[i]\n",
        "                v_string += f'{drive_val}-'\n",
        "\n",
        "            # handle using constructor as an interaction \n",
        "            if constructors[j] != \"any_constructor\":\n",
        "                interact *= data[constructors[j]].copy()\n",
        "                construct_val = constructors[j]\n",
        "                v_string += f'{construct_val}-'\n",
        "            \n",
        "            v_string += vars[0]\n",
        "            for k in range(1, len(vars)):\n",
        "                # print('loop executes?')\n",
        "                interact *= data[vars[k]].copy()\n",
        "                v_string += \"-{}\".format(vars[k])\n",
        "            \n",
        "            df = pd.DataFrame({\n",
        "                v_string: interact\n",
        "            })\n",
        "            data2 = pd.concat([data2, df], axis=1)\n",
        "            # # add interaction to the dataframe\n",
        "            # data[v_string] = interact\n",
        "\n",
        "    return data2"
      ],
      "id": "978f60a0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because there are so many interaction types to explore, we limit the scope of\n",
        "our search space to the following:\n",
        "\n",
        "+ Interactions between constructor, regulation, and cycle \n",
        "  variables\n",
        "  + constructor * engine_reg * cycle\n",
        "  + constructor * aero_reg * cycle\n",
        "  + constructor * years since major reg * round\n",
        "  + constructor * years since major reg * round * cycle\n",
        "  + constructor * round * cycle\n",
        "  + constructor * round\n",
        "  + constructor * round * years since major reg\n",
        "+ Interactions between constructor, track variables, and regulation\n",
        "  + constructor * min corner speed * aero_reg * cycle\n",
        "  + constructor * min corner speed * engine_reg * cycle\n",
        "  + constructor * min corner speed * cycle\n",
        "  + constructor * max corner speed * same three reg variables\n",
        "  + constructor * windspeed * cycle\n",
        "\n",
        "We only test the interactions for possible combinations."
      ],
      "id": "88eaa1eb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "'''\n",
        "we know that constructor will be interacted with everything, \n",
        "so just feed the same list every time. Leave drivers empty\n",
        "'''\n",
        "interactions = [\n",
        "  ['engine_reg','cycle_2'],\n",
        "  ['engine_reg','cycle_3'],\n",
        "  ['engine_reg','cycle_4'],\n",
        "  ['aero_reg','cycle_2'],\n",
        "  ['aero_reg','cycle_3'],\n",
        "  ['aero_reg','cycle_4'],\n",
        "  ['years_since_major_cycle','cycle_2'],\n",
        "  ['years_since_major_cycle','cycle_3'],\n",
        "  ['years_since_major_cycle','cycle_4'],\n",
        "  ['years_since_major_cycle','round','cycle_2'],\n",
        "  ['years_since_major_cycle','round','cycle_3'],\n",
        "  ['years_since_major_cycle','round','cycle_4'],\n",
        "  ['corner_spd_min','cycle_2','aero_reg'],\n",
        "  ['corner_spd_min','cycle_3','aero_reg'],\n",
        "  ['corner_spd_min','cycle_4','aero_reg'],\n",
        "  ['corner_spd_max','cycle_2','engine_reg'],\n",
        "  ['corner_spd_max','cycle_3','engine_reg'],\n",
        "  ['corner_spd_max','cycle_4','engine_reg'],\n",
        "  ['corner_spd_min','cycle_2'],\n",
        "  ['corner_spd_min','cycle_3'],\n",
        "  ['corner_spd_min','cycle_4'],\n",
        "  ['round','cycle_4'],\n",
        "  ['round'],\n",
        "  ['round', 'years_since_major_cycle'],\n",
        "  ['windspeed','cycle_4']\n",
        "]\n",
        "\n",
        "for interaction in interactions:\n",
        "    encoded_dat = add_interaction(encoded_dat, constructors=construct_vars, vars=interaction)"
      ],
      "id": "7c831344",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have added all of the meaningful interactions into the data, \n",
        "we can check which of them are actually useful using the select K best \n",
        "method. Before we do this, we must first finish encoding all of the \n",
        "categorical variables for processing:"
      ],
      "id": "a746b51d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# find all categorical variables\n",
        "encoded_dat.select_dtypes(include=['object']).columns"
      ],
      "id": "57e4c7cc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "None of these categorical variables are particularly useful for the \n",
        "task of predicting the target variables `positionOrder` and \n",
        "`quali_position`, so we can safely drop them from our dataset."
      ],
      "id": "54f2880a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "encoded_dat = encoded_dat.drop(['event_name', 'preciptype',\n",
        "                                'fastestLap', 'rank',\n",
        "                                'fastestLapTime', 'fastestLapSpeed',\n",
        "                                'ref_name', 'q1', 'q2', 'q3'],\n",
        "                                axis=1)\n",
        "\n",
        "# drop null values from miscellaneous columns (missing race tracks)\n",
        "encoded_dat = encoded_dat.dropna()\n",
        "\n",
        "# create the actual target variables of interest:\n",
        "encoded_dat.loc[(encoded_dat['positionOrder'] <= 3), 'top3r'] = 1\n",
        "encoded_dat.loc[(encoded_dat['positionOrder'] > 3), 'top3r'] = 0\n",
        "\n",
        "encoded_dat.loc[(encoded_dat['quali_position'] <= 3), 'top3q'] = 1\n",
        "encoded_dat.loc[(encoded_dat['quali_position'] > 3), 'top3q'] = 0"
      ],
      "id": "a1c0522a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before performing feature selection, let us make sure that we have removed all of \n",
        "the target variables (variables present only after qualifying has started)\n",
        "from the data"
      ],
      "id": "bfe55063"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data.columns"
      ],
      "id": "5dc051b2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking at the data, we can see that the `points` value are contained in this\n",
        "data as well. Thus, we should remove that variable in addition to teh `positionOrder`,\n",
        "`quali_position`, and `top3` and some other post-race informative variables. "
      ],
      "id": "3d00dea0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# get the output variables\n",
        "y_qual = encoded_dat['quali_position']\n",
        "y_race = encoded_dat['positionOrder']\n",
        "y_top3_finish = encoded_dat['top3r']\n",
        "y_top3_quali = encoded_dat['top3q']\n",
        "\n",
        "X = encoded_dat.drop(['quali_position','points','top3r','top3q', 'positionOrder',\n",
        "                      'statusId', 'grid', 'driverId', 'constructorId',\n",
        "                      'laps', 'resultId', 'regulation_id', 'year','raceId'], axis=1)"
      ],
      "id": "2d0475d0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Additionally, \n",
        "since the data is imbalanced (only 14% of the results are top3 finishes), we should make \n",
        "sure to use SMOTE to improve the balance of the dataset. We need to be careful of categorical\n",
        "predictors however, since those will throw off the selection performance quite drastically.\n",
        "We need to specify the column indices where categorical variables are held. Otherwise,\n",
        "the algorithm will automatically treat each variable as a continuous variable, and \n",
        "generate synthetic data accordingly - this would be unacceptable behavior.\n",
        "\n",
        "Because of how we have organized the data, we know that the only variables with explicitly \n",
        "binary results are interactions only between two categories, or the categories themselves.\n",
        "Namely, this means that we need to obtain the column indices of those which contain \n",
        "\"driverId\" or \"constructorId\" in them and no other values:"
      ],
      "id": "6feaa5e0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "driv_cons_cycles = []\n",
        "for column in X.columns:\n",
        "    if not ((\"constructorId\" in column) or (\"driverId\" in column) or ('cycle' in column)):\n",
        "        continue\n",
        "    if len(column.split(\"-\")) == 1:\n",
        "        driv_cons_cycles.append(column)\n",
        "\n",
        "cat_indices = []\n",
        "for col in driv_cons_cycles:\n",
        "    cat_indices.append(X.columns.get_loc(col))"
      ],
      "id": "b920e6a1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we have the categorical indices, so we can pass them to the SMOTENC module \n",
        "to avoid oversampling incorrectly."
      ],
      "id": "131c4ffa"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# rebalance the data with SMOTE\n",
        "from imblearn.over_sampling import SMOTENC\n",
        "\n",
        "oversample = SMOTENC(categorical_features=cat_indices, random_state=0)\n",
        "X2, y_top3_finish2 = oversample.fit_resample(X, y_top3_finish)"
      ],
      "id": "9265e7c4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have our variable sets, we can run the `mutual_info_classif()` method from \n",
        "`sklearn` to perform variable selection. This function is used because it \n",
        "automatically estimates the dependency between variables the target variable\n",
        "is discrete in nature. Since we are dealing with binary classification, it is \n",
        "appropriate to use this method."
      ],
      "id": "184d64bb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# import the package\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "mut_info_score = mutual_info_classif(X2, y_top3_finish2)"
      ],
      "id": "f1c30af3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we combine this with the columns of X to obtain a listing of the features\n",
        "we want:"
      ],
      "id": "214ffd78"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "scores = pd.DataFrame({\n",
        "  \"Feature\": X.columns,\n",
        "  \"Mutual Information\": mut_info_score\n",
        "})"
      ],
      "id": "57221d4b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can sort the dataframe in descending order to observe the top $k$ most\n",
        "important features"
      ],
      "id": "ac6359aa"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "scores = scores.sort_values(by='Mutual Information', ascending=False)\n",
        "scores.head(20)"
      ],
      "id": "2c470aea",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also use the select K best method to see how that might perform\n",
        "at this task:"
      ],
      "id": "a55788c0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "f_score, f_p_value = f_classif(X2, y_top3_finish2)\n",
        "f_scores = pd.DataFrame({\n",
        "  \"Feature\": X.columns,\n",
        "  \"F_score\": f_score,\n",
        "  \"P_value\": f_p_value\n",
        "})\n",
        "scores = pd.merge(f_scores, scores, on='Feature', how='inner')"
      ],
      "id": "32d77d07",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "sort by the best values selected from f_classif"
      ],
      "id": "ef5ad9cd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "scores = scores.sort_values(by='F_score', ascending=False)\n",
        "scores.head(30)"
      ],
      "id": "0d5ec8ba",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the way in which the fittings have resulted, it seems as if the mutual info \n",
        "placed too much emphasis upon the similarity between the weather covariates\n",
        "and the response. Although these variables should have an effect upon the \n",
        "race outcomes, their objective correlation to the response is low. \n",
        "As such, we should not prioritize these values when formulating a model. \n",
        "However, `f_classif` makes a much better determination in this regard. \n",
        "As such, we will use the selected classes provided by its run to construct our\n",
        "models.\n",
        "\n",
        "We can of course compare these results with those we obtain from the \n",
        "base data (no smote). We can use the features accumulated from each of\n",
        "these models to determine which constructs the most predictively \n",
        "accurate model."
      ],
      "id": "3b149a6a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "f_score, f_p_value = f_classif(X, y_top3_finish)\n",
        "mut_info = mutual_info_classif(X, y_top3_finish)\n",
        "\n",
        "imbalanced_scores = pd.DataFrame({\n",
        "  \"Feature\": X.columns,\n",
        "  \"F_score\": f_score,\n",
        "  \"P_value\": f_p_value,\n",
        "  \"mut_info\": mut_info\n",
        "})\n",
        "\n",
        "imbalanced_scores = imbalanced_scores.sort_values(by='F_score', ascending=False)\n",
        "imbalanced_scores.head(30)"
      ],
      "id": "ce9506af",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It seems that it is also better to determine the best\n",
        "features from the imbalanced (unexpanded) data. These are closer \n",
        "to the original class information, and are probably better\n",
        "for overall training. However, this does raise questions about the \n",
        "effectiveness of using SMOTE for model development in this case. \n",
        "However, we can compare a series of models to determine the overall\n",
        "highest performing method.\n",
        "\n",
        "# Model Development and Training\n",
        "\n",
        "Given our preliminary analysis, we have identified a series of \n",
        "models which will be useful for analysis. Since we want to compare both\n",
        "SMOTE and non-smote techniques, we need to extract a test set from the original\n",
        "data and use SMOTE exclusively for training. \n",
        "\n",
        "We use logistic regression in each case to model the problem (for consistency). \n",
        "Due to potentially strong collinearity between the covariates selected for \n",
        "by all methods, we will use L2 regularization to address codependencies of the \n",
        "involved features. However, is likely that random forrest models would perform \n",
        "strongly in this case due to the number of categorical predictors utilized by the \n",
        "data. \n",
        "\n",
        "For each method, we test three model sizes: k=10, k=20, and k=30\n",
        "and evaluate the validation performance of each on the test data\n",
        "\n",
        "SMOTE - using the top $k$ features selected from data \n",
        "where SMOTE has already been performed on the classification data.\n",
        "We perform this process both for mutual class info and f_classif metrics.\n",
        "\n",
        "NON-SMOTE - using the top $k$ using the top $k$ features selected from data \n",
        "where SMOTE has not yet been performed on the classification data.\n",
        "We perform this process both for mutual class info and f_classif metrics.\n",
        "\n",
        "## SMOTE Models\n",
        "\n",
        "We first import the scikit-learn packages necessary to train the \n",
        "models of interest"
      ],
      "id": "91a924a4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_recall_fscore_support"
      ],
      "id": "b96f3f22",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also need to create the universal train and test splits for the \n",
        "data to work off of"
      ],
      "id": "5aca7197"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# get the normal data\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y_top3_finish, test_size=0.2, random_state=42)\n",
        "\n",
        "# get the oversampled training data\n",
        "oversample = SMOTENC(categorical_features=cat_indices, random_state=0)\n",
        "x_train_smote, y_train_smote = oversample.fit_resample(x_train, y_train)"
      ],
      "id": "d1d2f686",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "define a function for training and validating a model automatically"
      ],
      "id": "7953d2e9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def train_eval(name:str, method:str, features:list):\n",
        "    '''\n",
        "    Assumes global access to the training and test data\n",
        "    '''\n",
        "    x_trn = None\n",
        "    y_trn = None\n",
        "    x_tst = x_test[features]\n",
        "    y_tst = y_test\n",
        "\n",
        "    if method==\"smote\":\n",
        "      x_trn = x_train_smote[features]\n",
        "      y_trn = y_train_smote\n",
        "    elif method == \"imbalanced\":\n",
        "      x_trn = x_train\n",
        "      y_trn = y_train\n",
        "\n",
        "    model = LogisticRegression(penalty='l2')\n",
        "    model.fit(x_trn, y_trn)\n",
        "    pred = model.predict(x_tst)\n",
        "    f1 = f1_score(y_tst, pred)\n",
        "    acc = accuracy_score(y_tst, pred)\n",
        "    precision, recall, fbeta_score, support = precision_recall_fscore_support(y_tst, pred)\n",
        "    return f1, acc, precision, recall, fbeta_score"
      ],
      "id": "bade0634",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "define the models to evaluate as a dictionary"
      ],
      "id": "f00396dc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "'''\n",
        "first index of each list entry is the type of training data to use\n",
        "the second entry is the list of features required by the model\n",
        "\n",
        "Models are labeled as \n",
        "<training-method _ selection-method _ numfeatures _ features-selected-from>\n",
        "'''\n",
        "smot_mut_scores = scores.sort_values(by='Mutual Information', ascending=False)\n",
        "smot_mut_scores = smot_mut_scores.reset_index()\n",
        "\n",
        "smot_f_scores = scores.sort_values(by=\"F_score\", ascending=False)\n",
        "smot_f_scores = smot_f_scores.reset_index()\n",
        "\n",
        "norm_mut_scores = imbalanced_scores.sort_values(by='mut_info', ascending=False)\n",
        "norm_mut_scores = norm_mut_scores.reset_index()\n",
        "\n",
        "norm_f_scores = imbalanced_scores.sort_values(by='F_score', ascending=False)\n",
        "norm_f_scores = norm_f_scores.reset_index()\n",
        "\n",
        "models = {\n",
        "  \"smot_mut_30_smot\":[\"smote\", smot_mut_scores.loc[:30, 'Feature']],\n",
        "  \"smot_mut_20_smot\":[\"smote\", smot_mut_scores.loc[:20, 'Feature']],\n",
        "  \"smot_mut_10_smot\":[\"smote\", smot_mut_scores.loc[:10, 'Feature']],\n",
        "  \"smot_mut_30_norm\":[\"smote\", norm_mut_scores.loc[:30, 'Feature']],\n",
        "  \"smot_mut_20_norm\":[\"smote\", norm_mut_scores.loc[:20, 'Feature']],\n",
        "  \"smot_mut_10_norm\":[\"smote\", norm_mut_scores.loc[:10, 'Feature']],\n",
        "  \"smot_f_30_smot\": [\"smote\", smot_f_scores.loc[:30, 'Feature']],\n",
        "  \"smot_f_20_smot\": [\"smote\", smot_f_scores.loc[:20, 'Feature']],\n",
        "  \"smot_f_10_smot\": [\"smote\", smot_f_scores.loc[:10, 'Feature']],\n",
        "  \"smot_f_30_norm\": [\"smote\", norm_f_scores.loc[:30, 'Feature']],\n",
        "  \"smot_f_20_norm\": [\"smote\", norm_f_scores.loc[:20, 'Feature']],\n",
        "  \"smot_f_10_norm\": [\"smote\", norm_f_scores.loc[:10, 'Feature']]\n",
        "}"
      ],
      "id": "64771e01",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate all models and generate results data"
      ],
      "id": "414a7c7d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# initialize a dictionary to store the results associate with each model\n",
        "model_results = {\n",
        "  \"model name\":[],\n",
        "  \"accuracy score\": [],\n",
        "  \"f1 score\": [],\n",
        "  \"precision\": [],\n",
        "  \"recall\": [],\n",
        "  \"F beta score\": []\n",
        "}\n",
        "\n",
        "for mod in models:\n",
        "    f1, acc, precision, recall, fbeta_score = train_eval(mod, models[mod][0], models[mod][1])\n",
        "    model_results['model name'].append(mod)\n",
        "    model_results['accuracy score'].append(acc)\n",
        "    model_results['f1 score'].append(f1)\n",
        "    model_results['precision'].append(precision)\n",
        "    model_results['recall'].append(recall)\n",
        "    model_results['F beta score'].append(fbeta_score)"
      ],
      "id": "6ffeb8dc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "convert the results into a viewable data frame"
      ],
      "id": "02a2df5c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "results = pd.DataFrame(model_results)\n",
        "results"
      ],
      "id": "205632a4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From these, we can see that the highest performing logistic regression \n",
        "model resulted from using a smote expanded training data and features\n",
        "selected for the smote data by `f_classif` with the top 20 most important\n",
        "features selected (where the selection data was preprocessed with smote).\n",
        "\n",
        "> NOTE: precision, recall, and F beat score have two values each. The first\n",
        "value is the score on predicting class $0$ - lower than 3rd place - and the \n",
        "econd value is the score on predicting class $1$ - finishing 3rd place or \n",
        "higher.\n",
        "\n",
        "We still need to compare these results against the data without any of the \n",
        "additional features however. "
      ],
      "id": "806f3c5b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "base_encoded_dat = get_features(base_data, ['constructorId','driverId'], select=False)\n",
        "\n",
        "base_encoded_dat = base_encoded_dat.drop(['event_name', 'preciptype',\n",
        "                                'fastestLap', 'rank',\n",
        "                                'fastestLapTime', 'fastestLapSpeed',\n",
        "                                'q1', 'q2', 'q3'],\n",
        "                                axis=1)\n",
        "\n",
        "driv_cons_cycles = []\n",
        "for column in X.columns:\n",
        "    if not ((\"constructorId\" in column) or (\"driverId\" in column)):\n",
        "        continue\n",
        "    if len(column.split('-')) == 1:\n",
        "        driv_cons_cycles.append(column)\n",
        "\n",
        "cat_indices = []\n",
        "for col in driv_cons_cycles:\n",
        "    cat_indices.append(X.columns.get_loc(col))\n",
        "\n",
        "# drop null values from miscellaneous columns (missing race tracks)\n",
        "base_encoded_dat = base_encoded_dat.dropna()\n",
        "\n",
        "# create the actual target variables of interest:\n",
        "base_encoded_dat.loc[(base_encoded_dat['positionOrder'] <= 3), 'top3r'] = 1\n",
        "base_encoded_dat.loc[(base_encoded_dat['positionOrder'] > 3), 'top3r'] = 0\n",
        "\n",
        "base_encoded_dat.loc[(base_encoded_dat['quali_position'] <= 3), 'top3q'] = 1\n",
        "base_encoded_dat.loc[(base_encoded_dat['quali_position'] > 3), 'top3q'] = 0\n",
        "\n",
        "y_qual = base_encoded_dat['quali_position']\n",
        "y_race = base_encoded_dat['positionOrder']\n",
        "y_top3_finish = base_encoded_dat['top3r']\n",
        "y_top3_quali = base_encoded_dat['top3q']\n",
        "\n",
        "X = base_encoded_dat.drop(['quali_position','points','top3r','top3q', \n",
        "                      'positionOrder',\n",
        "                      'statusId', 'grid', 'driverId', 'constructorId',\n",
        "                      'laps', 'resultId', 'year','raceId'], axis=1)\n",
        "\n",
        "# get the normal data\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y_top3_finish, test_size=0.2, random_state=42)\n",
        "\n",
        "# get the oversampled training data\n",
        "oversample = SMOTENC(categorical_features=cat_indices, random_state=0)\n",
        "x_train_smote, y_train_smote = oversample.fit_resample(x_train, y_train)\n",
        "\n",
        "f_score, f_p_value = f_classif(X, y_top3_finish)\n",
        "f_scores = pd.DataFrame({\n",
        "  \"Feature\": X.columns,\n",
        "  \"F_score\": f_score,\n",
        "  \"P_value\": f_p_value\n",
        "})\n",
        "\n",
        "f_scores = f_scores.sort_values(by='F_score', ascending=False)\n",
        "f_scores = f_scores.reset_index()\n",
        "features = f_scores.loc[:20, 'Feature']\n",
        "\n",
        "f1, acc, precision, recall, fbeta_score = train_eval(name='base', method='smote', features=features)\n",
        "\n",
        "base_results = pd.DataFrame({\n",
        "  \"model name\": [\"baseline model\"],\n",
        "  \"accuracy score\": [acc],\n",
        "  \"f1 score\": [f1],\n",
        "  \"precision\": [precision],\n",
        "  \"recall\": [recall],\n",
        "  \"F beta score\": [fbeta_score]\n",
        "})\n",
        "\n",
        "results = pd.concat([results,base_results], axis=0)\n",
        "\n",
        "# save all of the results to a csv file\n",
        "results.to_csv(\"model_results.csv\")"
      ],
      "id": "54b86408",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The f1 score and accuracy of this model were $0.538$ and $0.816$ respectively\n",
        "which is nearly $0.09$ points lower (f1) than that achieved by the best model\n",
        "with the additional features!\n",
        "\n",
        "If we look at the precision and recall scores specifically, we can \n",
        "see that our optimal model achieves a higher precision on predicting \n",
        "top 3 finishers. Additionally, we achieve a higher positive prediction \n",
        "capability (recall) than the baseline model. Actually, on every \n",
        "metric our optimal model outperforms the baseline. Moreover, this is \n",
        "not only true of the optimal model but of every other model we \n",
        "train as well. What we can see from these results is that our model\n",
        "demonstrates a tendency to overpredict positive instances - top3 finishing\n",
        "position and fits non-top 3 finishes reasonably well. "
      ],
      "id": "a6e6be5e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# save model to file\n",
        "def save_model(location, model, mod_name, input_features):\n",
        "    filename = \"{}/{}.pkl\".format(location, mod_name)\n",
        "    with open(filename, \"wb\") as f:\n",
        "        pickle.dump(model, f)\n",
        "    with open(\"{}/{}_features.pkl\".format(location, mod_name), \"wb\") as f2:\n",
        "        pickle.dump(input_features, f2)"
      ],
      "id": "1cdaed96",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The last thing we need to do is analyze the effect of the coefficients.\n",
        "Since we are using a logistic regression model, we can interpret the \n",
        "effect of the coefficients as: *for each unit increase in variable $x$,* \n",
        "*the log-odds of increases/decreases by a factor of $coeff$*. If we \n",
        "exponentiate every value $e^{coeff}$ then we can interpret these as\n",
        "the $e^{coeff}-1 \\cdot 100\\%$ change in the odds of finishing in the \n",
        "top 3 positions for a given race. \n",
        "\n",
        "> We needed to rerun the following code to get the correct data again:\n"
      ],
      "id": "d4403ff5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# get the output variables\n",
        "y_qual = encoded_dat['quali_position']\n",
        "y_race = encoded_dat['positionOrder']\n",
        "y_top3_finish = encoded_dat['top3r']\n",
        "y_top3_quali = encoded_dat['top3q']\n",
        "\n",
        "X = encoded_dat.drop(['quali_position','points','top3r','top3q','positionOrder',\n",
        "                      'statusId', 'grid', 'driverId', 'constructorId',\n",
        "                      'laps', 'resultId', 'regulation_id', 'year','raceId'], axis=1)\n",
        "\n",
        "driv_cons_cycles = []\n",
        "for column in X.columns:\n",
        "    if not ((\"constructorId\" in column) or (\"driverId\" in column) or ('cycle' in column)):\n",
        "        continue\n",
        "    if len(column.split()) == 1:\n",
        "        driv_cons_cycles.append(column)\n",
        "\n",
        "cat_indices = []\n",
        "for col in driv_cons_cycles:\n",
        "    cat_indices.append(X.columns.get_loc(col))\n",
        "\n",
        "# get the normal data\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y_top3_finish, test_size=0.2, random_state=42)\n",
        "\n",
        "# get the oversampled training data\n",
        "oversample = SMOTENC(categorical_features=cat_indices, random_state=0)\n",
        "x_train_smote, y_train_smote = oversample.fit_resample(x_train, y_train)\n",
        "\n",
        "feats = models[\"smot_f_20_smot\"][1]\n",
        "\n",
        "y_trn = y_train\n",
        "x_trn = x_train[feats]\n",
        "opt_model = LogisticRegression(penalty='l2')\n",
        "opt_model.fit(x_trn, y_trn)\n",
        "\n",
        "coefficients = opt_model.coef_"
      ],
      "id": "c141a2bc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "probs = []\n",
        "\n",
        "for coefficient in coefficients[0]:\n",
        "    if coefficient < 0: \n",
        "        val = -(1 - np.exp(coefficient))\n",
        "    elif coefficient > 0:\n",
        "        val = np.exp(coefficient) - 1 \n",
        "    probs.append(val)\n",
        "\n",
        "features = pd.DataFrame({\n",
        "  \"features\": x_trn.columns,\n",
        "  # \"coefficients\": coefficients[0],\n",
        "  \"odds increase / decrease\": probs\n",
        "})\n",
        "\n",
        "features"
      ],
      "id": "458b26d4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Primary Covariates**\n",
        "\n",
        "From here we can interpret the effect of each variable. The constructor\n",
        "position has the strongest effect upon the outcome. Namely, for every increase\n",
        "in the constructor's current standing, the odds of a result being in the\n",
        "top3 decreases by 32%. Moreover, we can see that for every increase in driver\n",
        "position (1 to 2 for example), the odds of finishing in the top 3 \n",
        "decreases by 8%. Because we use l2 regularization, it is likely that the\n",
        "the driver points and constructor points are driven to 0 because of the\n",
        "overlap between points and driver standings and positions. Additionally, \n",
        "points values tend to be quite high, so single point differences\n",
        "won't make such a significant difference. However, if you have, say,\n",
        "400 points, then the probability of a top 3 finish would increase by \n",
        "3.11%. Of course, this seems quite low, but the previous point about \n",
        "regularization still holds true here.\n",
        "\n",
        "Interestingly, the number of previous constructor wins demonstrates a negative\n",
        "relationship to the odds of finishing in the top 3. Specifically, for every\n",
        "win the constructor has, the probability of finishing in the top 3 decreases\n",
        "by 1.7%. This may be due to the collinearity between different variables\n",
        "in the data. Another potential reason for this is that drivers from the\n",
        "same team may collectively have a high number of wins, but a significantly\n",
        "different probability of finishing in the top 3. For example, in 2023, \n",
        "Max Verstappen won 19 out of 23 races which occurred. However, his teammate \n",
        "Sergio Perez often struggled to finish on the podium (outside of the top3)\n",
        "In contrast, for every additional win a driver has, the probability that \n",
        "they would finish in the top3 increases by 12%. This\n",
        "is significant, and also expected given our preliminary analysis.\n",
        "\n",
        "**Regulation and Track Focused Covariates**\n",
        "\n",
        "The constructor ID 131, associated with Mercedes, indicated a 9% increase\n",
        "in the odds of a top 3 finish. This is reasonable given that Mercedes\n",
        "dominated F1 from 2014 until 2021, 7 of the 13 years analyzed (2010-2023).\n",
        "Additionally, we can see that a unit increase in the interaction between \n",
        "mercedes, cycle 3 (2014-2021) and years since major cycle led to a decrease\n",
        "in the odds of a top 3 finish by 7%. This is also reasonable since the\n",
        "dominance of Mercedes as a team began to slip as the turbo-hybrid engine\n",
        "regulations progressed. This culminated with their eventual defeat\n",
        "at the hands of Red Bull in 2021. We can also see that unit increases\n",
        "in the interaction between Mercedes and the competition round (race x out of\n",
        "24 for a given season) led to a 5.9% drop in the odds of a top 3 finish. \n",
        "\n",
        "As far as track specific variables are concerned, the most significant\n",
        "of these seemed to be the interaction between a track's minimum corner speed,\n",
        "Mercedes, and cycle 3 (2014-2021). In other words, for every unit increase\n",
        "in the minimum corner speed for a track (a faster track overall) which\n",
        "Mercedes drove at from 2014 to 2021, the odds of a top 3 finish \n",
        "increased by 2.37%. For example, the model indicates that Mercedes would\n",
        "have a 256% increase in odds of a top 3 finish at a track with a minimum \n",
        "corner speed of 174km/h versus a track with a minimum corner speed of 66km/h.\n",
        "These are the maximum and minimum (min corner speeds) respectively from \n",
        "the data.\n",
        "\n",
        "We can also see that unit increases in the interaction between constructor 5 \n",
        "and round (as the round increased for constructor 5) led to a 12% \n",
        "decrease in the odds of a top 3 finish for the constructor. This indicates\n",
        "that the constructor (Toro Rosso) would perform worse over the course of a \n",
        "given season. ConstructorID 10 (Force India) also indicated a decreased odds \n",
        "of a top 3 finish (decrease of 10.6%).\n",
        "\n",
        "## Caveats of Analysis, Conclusions, and Future Directions\n",
        "\n",
        "*Caveats*\n",
        "\n",
        "Because of the large number of potential features, especially\n",
        "interactions which could have been fit, it is hard to say with any certainty\n",
        "which would have been the most significant or useful. Additionally, the \n",
        "model itself fit more strongly to features which did not appear to be\n",
        "especially significant based on the results of the feature selection \n",
        "process.\n",
        "\n",
        "Additionally, further cross-validation would be required to fully \n",
        "confirm the results of this analysis. However, the performance improvement\n",
        "seen across all of the models would seem to indicate the significance\n",
        "of the added variables studied in this analysis.\n",
        "\n",
        "*Conclusions and Future Directions*\n",
        "\n",
        "This work indicates that track and regulation cycle based variables\n",
        "can be predictively useful, even when accounting for prior constructor\n",
        "and driver standings (the most significant). Additionally, this highlights\n",
        "the importance of using time-relevant data. Although our model is fit\n",
        "reasonably well for thisd ata, it may not perform so strongly \n",
        "on more recent data. As such, future models would do well to take advantage\n",
        "of more recent samples. Limited data may make it difficult to fit a robust \n",
        "model however. \n",
        "\n",
        "Additionally, alternative methods such as random forest classification, \n",
        "xgboosting, and deep learning will likely outperform logistic regression.\n",
        "Tree-based models make classifications based off of structured dependencies\n",
        "within the data. This means that such a model would likely be able to find\n",
        "more optimal interactions between features. Such an approach would eliminate\n",
        "the need for manual feature definition as was required here. Deep learning\n",
        "methods may also be more capable at learning the important interactions\n",
        "and dependencies between covariates. However, due to the limited amount of\n",
        "suitable data, it is difficult to say whether this method would outperform\n",
        "random forests or not.\n",
        "\n",
        "Future works would also do well to incorporate more data from \n",
        "weekend based data. F1-weekends include free-practice sessions which \n",
        "provide some insights into the true performance of the cars. Explicit\n",
        "session rankings have proven inaccurate in the past, but more nuanced\n",
        "analysis of lap times, tires used, and more may prove useful for \n",
        "predicting qualifying and race performances. \n",
        "\n",
        "**save the best model**"
      ],
      "id": "f144d25d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pickle \n",
        "\n",
        "save_model(\"pretrained\", opt_model, \"2010-2023_opt_logit\", x_trn.columns)"
      ],
      "id": "8b73f652",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}