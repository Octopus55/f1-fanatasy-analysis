[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "F1-race-predictions",
    "section": "",
    "text": "Welcome to F1-race-predictions! See predictions for the most recent race predictions. If you would like more information about the modeling process used, see about\nSee the official F1 website for more information about the 2024 race schedule, standings, and more: f1.com"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "F1 is a racing series where the best drivers and teams from across the world come together to compete. In fact, F1 is widely recognized as the most prestigious motorsport series in the world. This series has been running for a long time, but has recently experienced an explosion in popularity due to the publication of limited run tv-series, “Drive to Survive” as well as the extended marketing reach provided by social media platforms such as YouTube, Twitter, Instagram, and Reddit (just to name a few).\nAs the sport has continued to grow, so has the sports betting and fantasy sports scene. For example, on Reddit, users compete in a series of race-by-race prediction challenges. In these challenges, participants predict race-weekend outcomes such as pole position qualifier (which driver achieved the fastest lap time during qualifying), race winner, and other relevant statistics. Moreover, F1 itself provides a fantasy sports competition where users construct teams from the available manufacturers and drivers (with certain restrictions) to achieve the highest point totals over the course of a given year.\nNaturally, this extends to sports betting where the prediction of race outcomes and driver performances can have significant financial implications.\nA significant amount of research has been conducted towards this end. Namely, autocoder-decoder and MLP networks have been developed for rank position forecasting, Bayesian regression methods developed for predicting race finishing positions, and analytical approaches constructed to account for the combined effect of both driver and car on race performances.\nThis website has been designed to automatically generate predictions for each race of the 2024 F1 season and potentially beyond (depends on whether I decide to provide continued support for it or not)."
  },
  {
    "objectID": "about.html#predictions-and-problem-formulation",
    "href": "about.html#predictions-and-problem-formulation",
    "title": "About",
    "section": "Predictions and Problem Formulation",
    "text": "Predictions and Problem Formulation\nTo simplify the problem of race outcome prediction, I established two classes: top 3 and bottom 17. In other words, if a driver is predicted to finish in the top 3 positions (on the podium), then they would be classified as “top 3”, otherwise they would be classified as “bottom 17”. To model this binary classification problem, I used logistic regression under the following setting:\n\ndata from the fastf1 api used for generating track summary information\n\nminimum corner speed\naverage track speed\netc.\n\ndata from the visual crossing api used for generating weather information (windspeed)\nergast data compiled on kaggle for the F1 2022 and 2023 seasons\nsubjective regulation change significance - aero_reg\n\nthe higher these scores, the more significant the regulation change was which occurred going into a particular season of F1\n\n\nBecause the modeling problem is imbalanced (14% of results are top 3 finishes), I used SMOTE (synthetic minority over-sampling technique) in order to improve the balance of the data. F-scores were then obtained for the signficance of each candidate feature and the top 30 features were fitted for the final model. A test train split of 80-20 was used.\nThis model achieves an accuracy of 0.8971, and an f1-score of 0.7308. The features, their coefficients, and the odds-increase / decrease associated with each are provided as follows:\n\n\n\n\n\n\n\n\n\nfeatures\ncoefficients\nodds increase / decrease\n\n\n\n\n7\nprev_driver_position\n-0.169095\n-0.155571\n\n\n8\nprev_construct_position\n-0.089435\n-0.085553\n\n\n10\nconstructorId_9-strt_len_median\n-0.011137\n-0.011075\n\n\n13\nprev_construct_points\n-0.007542\n-0.007513\n\n\n30\ndriverId_844-aero_reg\n-0.006534\n-0.006512\n\n\n5\nconstructorId_9-avg_track_spd\n-0.003273\n-0.003268\n\n\n6\nconstructorId_9-corner_spd_min\n-0.003105\n-0.003100\n\n\n11\nconstructorId_9-windspeed\n-0.001691\n-0.001690\n\n\n25\ndriverId_815\n-0.001439\n-0.001438\n\n\n21\nconstructorId_9-corner_spd_min-aero_reg\n-0.000762\n-0.000762\n\n\n24\nconstructorId_9-years_since_major_cycle-round\n-0.000129\n-0.000129\n\n\n23\nconstructorId_9-round-years_since_major_cycle\n-0.000129\n-0.000129\n\n\n22\nconstructorId_9-years_since_major_cycle\n0.000005\n0.000005\n\n\n9\nconstructorId_9-strt_len_max\n0.000255\n0.000255\n\n\n4\nconstructorId_9-corner_spd_max\n0.000286\n0.000286\n\n\n2\nconstructorId_9\n0.000595\n0.000595\n\n\n17\nconstructorId_9-num_fast_corners\n0.000747\n0.000747\n\n\n18\nconstructorId_9-aero_reg\n0.000825\n0.000825\n\n\n1\nconstructorId_9-circuit_len\n0.000994\n0.000995\n\n\n27\nconstructorId_6-corner_spd_min-aero_reg\n0.001186\n0.001187\n\n\n14\nconstructorId_9-num_slow_corners\n0.001399\n0.001400\n\n\n29\nconstructorId_131-corner_spd_min-aero_reg\n0.001531\n0.001533\n\n\n15\ndriverId_830\n0.002034\n0.002036\n\n\n0\nconstructorId_9-num_corners\n0.002251\n0.002254\n\n\n16\nprev_construct_wins\n0.003298\n0.003304\n\n\n19\nprev_driver_wins\n0.003342\n0.003348\n\n\n20\nconstructorId_9-round\n0.003805\n0.003813\n\n\n28\ndriverId_844-strt_len_median\n0.004471\n0.004481\n\n\n3\nconstructorId_9-max_track_spd\n0.006688\n0.006711\n\n\n26\nconstructorId_6-aero_reg\n0.008188\n0.008221\n\n\n12\nprev_driver_points\n0.014778\n0.014887\n\n\n\n\n\n\n\nNote that constructor and driver ids listed correspond to the following:\n\nconstructorId_9: Red Bull Racing\nconstructorId_6: Ferrari\nconstructorId_131: Mercedes\ndriverId_830: Max Verstappen\ndriverId_844: Charles Leclerc\ndriverId_815: Sergio Perez"
  },
  {
    "objectID": "honors/outcomes.html",
    "href": "honors/outcomes.html",
    "title": "F1-Analysis for the 2024 British Grand Prix",
    "section": "",
    "text": "track layout and speeds\n\n\nAnalysis for conducted using a logistic regression with 30 features for additional information about the modeling process, please send me a message on GitHub issues"
  },
  {
    "objectID": "honors/outcomes.html#track-layout-information",
    "href": "honors/outcomes.html#track-layout-information",
    "title": "F1-Analysis for the 2024 British Grand Prix",
    "section": "",
    "text": "track layout and speeds\n\n\nAnalysis for conducted using a logistic regression with 30 features for additional information about the modeling process, please send me a message on GitHub issues"
  },
  {
    "objectID": "honors/outcomes.html#top3-finishing-position-predictions",
    "href": "honors/outcomes.html#top3-finishing-position-predictions",
    "title": "F1-Analysis for the 2024 British Grand Prix",
    "section": "Top3 Finishing Position Predictions",
    "text": "Top3 Finishing Position Predictions\nThe table below presents odds of placing in the top 3 positions for each of the drivers in the current standings for F1.\n\n\n\n\n\n\n\n\n\nrank\nprob of top 3 finish\nprob of bottom 17 finish\ndriver name\n\n\n\n\n0\n5\n0.995088\n0.004912\nMax Verstappen\n\n\n1\n7\n0.945123\n0.054877\nSergio Perez\n\n\n2\n11\n0.630317\n0.369683\nCharles Leclerc\n\n\n3\n20\n0.425242\n0.574758\nLando Norris\n\n\n4\n1\n0.331598\n0.668402\nGeorge Russell\n\n\n5\n3\n0.324426\n0.675574\nCarlos Sainz\n\n\n6\n4\n0.160445\n0.839555\nLewis Hamilton\n\n\n7\n18\n0.140453\n0.859547\nFernando Alonso\n\n\n8\n2\n0.136502\n0.863498\nOscar Piastri\n\n\n9\n14\n0.104002\n0.895998\nYuki Tsunoda\n\n\n10\n13\n0.075551\n0.924449\nLance Stroll\n\n\n11\n6\n0.068050\n0.931950\nNico Hulkenberg\n\n\n12\n9\n0.055297\n0.944703\nDaniel Ricciardo\n\n\n13\n10\n0.044235\n0.955765\nPierre Gasly\n\n\n14\n8\n0.038134\n0.961866\nKevin Magnussen\n\n\n15\n12\n0.030605\n0.969395\nEsteban Ocon\n\n\n16\n15\n0.024698\n0.975302\nAlexander Albon\n\n\n17\n16\n0.018909\n0.981091\nValtteri Bottas\n\n\n18\n17\n0.016014\n0.983986\nGuanyu Zhou\n\n\n19\n19\n0.014588\n0.985412\nLogan Sargeant\n\n\n\n\n\n\n\nWe can see here that the top 3 predicted finishers are:\n\nMax Verstappen : probability of top 3 finish - 99.51%\nSergio Perez : probability of top 3 finish - 94.51%\nCharles Leclerc : probability of top 3 finish - 63.03%"
  },
  {
    "objectID": "manuscript/report.html",
    "href": "manuscript/report.html",
    "title": "STAT4255 Final Project: F1 Race Podium Forecasting",
    "section": "",
    "text": "The Formula 1 dataset we used has 72 features where each observation represents the results of 1 driver in a specific race. For example, if a race has 20 participating drivers, then there will be 20 records associated with that specific race. Many of the features are related to characteristics of a track such as the average speed, the maximum speed, the longest staight length, the number of corners, etc. Other features tell information about the track conditions such as temperature, humidity, precipitation, and windspeed. Lastly, some of the most important features have to do with how a driver or team has performed in previous races. These features convey performance through how many points or wins a driver or team has accumulated in a span of prior races.\nThe main question we wanted to tackle was determining what training size is best for predicting podium (top 3) finishes for each driver. To accomplish this we had to fit models for various training sizes (also called windows) of prior races and then compare how well they performed in classifying an observation as a podium finish or not in the next race. This question was especially interesting because in previous analyses, overfitting to the training data was a common source of error. F1 teams are capable of significantly upgrading their cars from race to race which can be difficult for models to adapt to. The aim of this question was to pinpoint the window size for which the training data is most most relevant. A model with a window size of just a handful of races (say 2 or 3) may not have enough information to make accurate predictions and put too much weight in a small sample of results. On the other hand, a model with a large window size (15 or 20 races) could be basing its predictions off of noisy data that is no longer representative of the current competition. We wanted to know what window size is the “sweet spot” for making the most accurate predictions."
  },
  {
    "objectID": "manuscript/report.html#experimental-results",
    "href": "manuscript/report.html#experimental-results",
    "title": "STAT4255 Final Project: F1 Race Podium Forecasting",
    "section": "Experimental Results",
    "text": "Experimental Results\nParameterizing the training data window size\nIn order to validate our approach, we performed several experiments over the data. First, we thoroughly investigated the performance of the XGBoost model over 2024 F1 race data (races spanning from Bahrain 2024 to Qatar 2024) for a fixed selection of features, namely:\n\nThe proportion of points in the standings belonging to each driver before a given race\nWhether or not a driver drove for McLaren\nWhether or not a driver drove for RedBull\nWhether or not a driver was Charles Leclerc\nThe maximum track speed for a given circuit\nThe number of previous constructor wins\nThe number of podiums scored by a given driver over the previous 2, 3, and 4 races\n\nUsing these features, we fitted XGBoost models over data window sizes of 2 to 11 rounds. Because XGBoost’s fitting algorithm is stochastic, we needed to perform multiple runs to generate a confidence interval over the true F1 score of the model on the data. From a series of initial runs, we determined that 7 races was the strongest data window size, and then ran 15 iterations to estimate the standard errors of this estimate. Using CLT, we can approximate the distribution of mean F1 scores for training on 7 races of data by a normal distribution (as long as the sample size is sufficiently large).\nNamely, for training an XGBoosing model over data from the previous 7 races, we observe an average F1 score of 0.536 over the next race in sequence (for all races from Bahrain to Qatar). The standard deviation of average F1 scores achieved by these models was 0.0072, and so a 95% confidence interval for an estimate of the average F1 score achieved by XGBoost trainged over the prior 7 races of data in 2024 was\n\\[\n[0.5224, 0.5506]\n\\]\nThis is a nice bound, since it means that our XGBoost model typically performs well for race data windows of size 7 (better than random guessing). To see that seven is the best choice, the results for window sizes from 2 to 11 can be seen below:\n\nimport pandas as pd\n\nres = pd.read_csv(\"../code/experiments/results_window_sample.csv\")\nres\n\n\n\n\n\n\n\n\nn\nAccuracy\nF1 Score\nAcc_Max\nAcc_Min\nF1_Max\nF1_Min\n\n\n\n\n0\n2.0\n0.837594\n0.460317\n1.0\n0.700000\n1.000000\n0.000000\n\n\n1\n3.0\n0.842356\n0.476190\n1.0\n0.700000\n1.000000\n0.000000\n\n\n2\n4.0\n0.870927\n0.571429\n1.0\n0.700000\n1.000000\n0.000000\n\n\n3\n5.0\n0.856642\n0.523810\n1.0\n0.789474\n1.000000\n0.333333\n\n\n4\n6.0\n0.866416\n0.555556\n1.0\n0.800000\n1.000000\n0.333333\n\n\n5\n7.0\n0.856642\n0.523810\n0.9\n0.789474\n0.666667\n0.333333\n\n\n6\n8.0\n0.847118\n0.492063\n1.0\n0.789474\n1.000000\n0.333333\n\n\n7\n9.0\n0.837594\n0.460317\n0.9\n0.700000\n0.666667\n0.000000\n\n\n8\n10.0\n0.832832\n0.444444\n0.9\n0.700000\n0.666667\n0.000000\n\n\n9\n11.0\n0.813784\n0.380952\n0.9\n0.700000\n0.666667\n0.000000\n\n\n10\n12.0\n0.823308\n0.412698\n0.9\n0.700000\n0.666667\n0.000000\n\n\n\n\n\n\n\n\nThe utility of track features\nOn their own, track features are not explicitly useful. The reason for this is that for a given track the track features will be the same for all drivers. As such, if we do not identify the drivers in question, the podium outcomes will be randomly associated with the track features themselves. Thus, we include the driver and constructor information to interact with these variables to achieve improved performance outcomes.\nNamely, we consider McLaren and Red Bull as teams and Charles Leclerc as a individual driver for interaction with track features. To determine if these features make a meaningful performance contribution, we compare baseline models which do and do not include track features. Unfortunately, we find that often times including track features actually diminished performance on the whole. Perhaps the reason for this was that different track features are differentially useful across a given race season, and so the average F1 score fitted on models for features that are not useful over all possible windows adds unecessary noise and diminishes performance.\nThe utility of scaled point distributions\nTo determine if scaling the point distribution was useful or not, we compared the performance of XGboost models fitted only on raw previous driver point totals and scaled previous driver point totals (driver scaled points = driver total points / number of points awarded to all drivers).\nBelow you can see the comparative performance of XGboost models trained over data from 2 to 9 of the previous races on the next race in sequence when scaled and unscaled driver point totals are given:\nRaw (Unscaled) point totals\n\nunscaled_pts = pd.read_csv(\"../code/experiments/un-scaled_points_sample.csv\")\nunscaled_pts\n\n\n\n\n\n\n\n\nn\nAccuracy\nF1 Score\nAcc_Max\nAcc_Min\nF1_Max\nF1_Min\n\n\n\n\n0\n2.0\n0.775439\n0.253968\n0.9\n0.684211\n0.666667\n0.0\n\n\n1\n3.0\n0.785213\n0.285714\n0.9\n0.700000\n0.666667\n0.0\n\n\n2\n4.0\n0.794987\n0.317460\n0.9\n0.700000\n0.666667\n0.0\n\n\n3\n5.0\n0.799749\n0.333333\n0.9\n0.700000\n0.666667\n0.0\n\n\n4\n6.0\n0.799499\n0.333333\n1.0\n0.700000\n1.000000\n0.0\n\n\n5\n7.0\n0.785213\n0.285714\n0.9\n0.700000\n0.666667\n0.0\n\n\n6\n8.0\n0.780702\n0.269841\n0.9\n0.700000\n0.666667\n0.0\n\n\n7\n9.0\n0.771178\n0.238095\n0.9\n0.700000\n0.666667\n0.0\n\n\n8\n10.0\n0.780702\n0.269841\n0.9\n0.700000\n0.666667\n0.0\n\n\n\n\n\n\n\nScaled point totals\n\nscaled_pts = pd.read_csv(\"../code/experiments/scaled_points_sample.csv\")\nscaled_pts\n\n\n\n\n\n\n\n\nn\nAccuracy\nF1 Score\nAcc_Max\nAcc_Min\nF1_Max\nF1_Min\n\n\n\n\n0\n2.0\n0.794737\n0.317460\n0.9\n0.7\n0.666667\n0.0\n\n\n1\n3.0\n0.813784\n0.380952\n0.9\n0.7\n0.666667\n0.0\n\n\n2\n4.0\n0.818546\n0.396825\n0.9\n0.7\n0.666667\n0.0\n\n\n3\n5.0\n0.813784\n0.380952\n0.9\n0.7\n0.666667\n0.0\n\n\n4\n6.0\n0.809023\n0.365079\n1.0\n0.7\n1.000000\n0.0\n\n\n5\n7.0\n0.813784\n0.380952\n0.9\n0.7\n0.666667\n0.0\n\n\n6\n8.0\n0.828070\n0.428571\n0.9\n0.7\n0.666667\n0.0\n\n\n7\n9.0\n0.799499\n0.333333\n0.9\n0.7\n0.666667\n0.0\n\n\n8\n10.0\n0.823308\n0.412698\n0.9\n0.7\n0.666667\n0.0\n\n\n\n\n\n\n\nAs you can see, F1 scores improve substantially when using a scaled point distribution, indicating that this change was predictively beneficial."
  }
]